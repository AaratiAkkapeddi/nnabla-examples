{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StyleGAN2\n",
        "\n",
        "![styleGAN2 generated image sample](https://github.com/sony/nnabla-examples/raw/master/GANs/stylegan2/images/sample.png)\n",
        "\n",
        "This example demonstrates face image generation using [StyleGAN2](https://github.com/NVlabs/stylegan2). StyleGAN2 is one of the generative models which can generate high-resolution images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime â†’ change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples/GANs/stylegan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get the pretrained weights\n",
        "Now we will get the pretrained weights for styleGAN2, then import some modules and do some preparation for the latter part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/GANs/stylegan2/styleGAN2_G_params.h5\n",
        "from generate import *\n",
        "from IPython.display import Image, display\n",
        "ctx = get_extension_context(\"cudnn\")\n",
        "nn.set_default_context(ctx)\n",
        "\n",
        "batch_size = 1\n",
        "num_layers = 18\n",
        "\n",
        "nn.load_parameters(\"styleGAN2_G_params.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StyleGAN2 input config\n",
        "\n",
        "In styleGAN2, the noise input **z** is fed to the **mapping network** to produce the latent code **w**. Then **w** is modified via **truncation trick** and finally the modified latent code **w'** is injected to the **synthesis network**.\n",
        "\n",
        "With multiple latent codes **w'** coming from the **mapping network**, **synthesis network** transforms the incoming tensor and gradually converts it to an image. \n",
        "\n",
        "This is how styleGAN2 generates photo-realistic high resolution images. \n",
        "\n",
        "In the following cell,  you will choose the random seed used for sampling the noise input **z**, the value for **truncation trick**, and another random seed used for the additional noise input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@markdown Choose the seed for noise input **z**. (This drastically changes the result)\n",
        "latent_seed = 600  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 0.5  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown Choose the seed for stochasticity input.  (This slightly changes the result)\n",
        "noise_seed = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Now let's run StyleGAN2!\n",
        "Execution the following cell will run the styleGAN2. You can see by changing the value used for **truncation trick**, you will get the different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rnd = np.random.RandomState(latent_seed)\n",
        "z = rnd.randn(batch_size, 512)\n",
        "\n",
        "style_noise = nn.Variable((batch_size, 512)).apply(d=z)\n",
        "style_noises = [style_noise for _ in range(num_layers)]\n",
        "\n",
        "rgb_output = generate(batch_size, style_noises, noise_seed, truncation_psi)\n",
        "rgb_output.forward()\n",
        "\n",
        "image = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "filename = f\"seed{latent_seed}.png\"\n",
        "imsave(filename, image, channel_first=True)\n",
        "\n",
        "display(Image(filename, width=512, height=512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Try Style Mixing\n",
        "\n",
        "![styleGAN2 generated image sample](https://github.com/sony/nnabla-examples/raw/master/GANs/stylegan2/images/style_mixing_sample.png)\n",
        "\n",
        "As described above, in styleGAN2, **synthesis network** receives latent code **w** multiple times and generates images. In the previous generation, latent code **w** which **synthesis network** receives is made from one single noise input **z**. In this case, we can say that **w** controls the *style* of the generated image.\n",
        "\n",
        "Given that, with a *different* latent code **w2**, made from another noise input **z2**, **synthesis network** can generate a completely different image. So, what if we use both **w** and **w2**...? That is, *style mixing*.\n",
        "\n",
        "To be specific, using 2 latent codes **w** and **w2**, **synthesis network** can generate the image which contains both elements (i.e. hair style, face components), present in images made from **w** (controling coarse style) and **w2** (controling fine style).\n",
        "\n",
        "In the following cell, you will choose one more random seed used for sampling another noise input **z2**. \n",
        "\n",
        "You can also choose from which layer it receives the additional latent code **w2**. It slightly changes the result, so try various patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title StyleGAN2 style mixing config\n",
        "#@markdown Choose seed for the primary noise input **z**.\n",
        "latent_seed = 600  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose seed for the secondary noise input **z2**.\n",
        "latent_seed2 = 300  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose from which layer to use the secondary latent code **w2**.\n",
        "mix_after = 7  #@param {type: \"slider\", min: 0, max: 17, step:1}\n",
        "\n",
        "#@markdown Choose seed for stochasticity input.\n",
        "noise_seed = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 0.5  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown ---\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's run style mixing.\n",
        "\n",
        "Running this cell executes style mixing and displays a generated mixed image and images made solely from **w** / **w2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rnd = np.random.RandomState(latent_seed)\n",
        "z = rnd.randn(batch_size, 512)\n",
        "\n",
        "rnd2 = np.random.RandomState(latent_seed2)\n",
        "z2 = rnd2.randn(batch_size, 512)\n",
        "\n",
        "style_noises = [nn.Variable((batch_size, 512)).apply(d=z) for _ in range(mix_after)]\n",
        "style_noises += [nn.Variable((batch_size, 512)).apply(d=z2) for _ in range(num_layers - mix_after)]\n",
        "\n",
        "rgb_output = generate(batch_size, style_noises, noise_seed, truncation_psi)\n",
        "rgb_output.forward()\n",
        "\n",
        "image_mix = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "for style_noise in style_noises:\n",
        "    style_noise.d = z\n",
        "rgb_output.forward()\n",
        "image_A = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "for style_noise in style_noises:\n",
        "    style_noise.d = z2\n",
        "rgb_output.forward()\n",
        "image_B = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "top_image = 255 * np.ones(image_mix.shape).astype(np.uint8)\n",
        "top_image = np.concatenate([top_image, image_B], axis=2)\n",
        "bottom_image = np.concatenate([image_A, image_mix], axis=2)\n",
        "grid_image = np.concatenate([top_image, bottom_image], axis=1)\n",
        "imsave(\"grid.png\", grid_image, channel_first=True)\n",
        "display(Image(\"grid.png\", width=512, height=512))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
